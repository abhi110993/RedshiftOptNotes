Copy Command Redshift

1. Copy job: (new feature)
 Autoamtically copy data from S3 to Redshift
2. Copy command: Mannualy execution of copy command
3. Automation: Triggering lambda on data ingestion. Lambda initiated copy command for Redshift


Limitations:

1. Copy command doesnot work, if you are overriding an existing file
2. 



Question:
1. What is the format of file?
2. Is it a single table?

What to do ?
1. Create redshift cluster(can create in preview mode) 
2. Create table using redshift console



What is the maximum size of copy in S3?
5 TB
Note. You can store individual objects of up to 5 TB in Amazon S3. 
You create a copy of your object up to 5 GB in size in a single atomic action using this API. 
However, to copy an object greater than 5 GB, you must use the multipart upload Upload Part - Copy (UploadPartCopy) API.

1. The maximum size of a single input row from any source is 4 MB.

2. A Redshift data warehouse cluster can contain from 1-128 compute nodes, depending on the node type.



GUIDE: https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data.html

Recommanded:
1. If the number of nodes are n, then total files recommended is n*8
2. The users need to be very careful about the implicit data type conversions that the Command does in the background. Otherwise, it could lead to a corrupted database.
3. The maximum size of a single input row from any source is 4 MB.
4. A Redshift data warehouse cluster can contain from 1-128 compute nodes, depending on the node type.
5. Redshift allows up to 16 petabytes of data on a cluster
6. The total volume of data and number of objects you can store in Amazon S3 are unlimited. 
7. Individual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 TB. 
	The largest object that can be uploaded in a single PUT is 5 GB.
8.
Maximum record size 	
20MB

Table name length 	
127 characters

Column name length 	
115 characters

Maximum columns per table 	
1,600

Maximum table size 	
None

Maximum tables per database 	
100,000

Case sensitivity 	
Insensitive
9. 


COST:
1000$ per terabyte per year

Data loading:
1. Amazon can not handle unstructured data
2. RUN VACCUM to reclaim space and resort rows
3. Run ANALYSE to update the stats info related to the tables.
4. Slots can be configured to run parallely and return the result
5. The results are stored in the upper layer of memory using LRU fashion
6. Ideally you should split the data and then use copy command to make efficient use of parallelisation
	#Split = #Slices in the cluster
7. After compression it is noted that best performance is in between 1 to 125 MB.
	Acc. to amazon its till 1GB only
8. The no. of files = n* #slices where 'n' is a natural number
9. Compress your data file.. e.g. LZO compression
10. Use a manifest file:
		"from s3://mybucket/custdata"
		it will load all the data with prefix custdata eg. custdata_log, custdata_1, custdata_2, etc
		but we did not want logs to be there
11. Maximum level of concurrency that can be acheived by the Redshift to execute a query is 50.
12. 





